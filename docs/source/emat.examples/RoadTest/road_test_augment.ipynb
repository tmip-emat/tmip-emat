{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. py:currentmodule:: emat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emat\n",
    "emat.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ema_workbench\n",
    "import os, numpy, pandas, functools\n",
    "from emat.util.xmle import Show\n",
    "from emat.viz.scatter import scatter_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = emat.util.loggers.log_to_stderr(20, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Exploratory Scope"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "The model scope is defined in a YAML file.  For this Road Test example, the scope file is named \n",
    ":ref:`road_test.yaml <road_test_scope_file>` and is included in the model/tests directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_test_scope_file = emat.package_file('model','tests','road_test.yaml')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "The filename for the YAML file is the first argument when creating a :class:`Scope`\n",
    "object, which will load and process the content of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_scope = emat.Scope(road_test_scope_file)\n",
    "road_scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A short summary of the scope can be reviewed using the `info` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_scope.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, more detailed information about each part of the scope can be\n",
    "accessed in four list attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_scope.get_constants()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_scope.get_uncertainties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_scope.get_levers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_scope.get_measures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Database\n",
    "\n",
    "The exploratory modeling process will typically generate many different sets of outputs,\n",
    "for different explored modeling scopes, or for different applications.  It is convenient\n",
    "to organize these outputs in a database structure, so they are stored consistently and \n",
    "readily available for subsequent analysis.\n",
    "\n",
    "The `SQLiteDB` object will create a database to store results.  When instantiated with\n",
    "no arguments, the database is initialized in-memory, which will not store anything to\n",
    "disk (which is convenient for this example, but in practice you will generally want to\n",
    "store data to disk so that it can persist after this Python session ends)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emat_db = emat.SQLiteDB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An EMAT Scope can be stored in the database, to provide needed information about what the \n",
    "various inputs and outputs represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_scope.store_scope(emat_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to store another scope with the same name (or the same scope) raises a KeyError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    road_scope.store_scope(emat_db)\n",
    "except KeyError as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can review the names of scopes already stored in the database using the `read_scope_names` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emat_db.read_scope_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Design\n",
    "\n",
    "Actually running the model can be done by the user on an *ad hoc* basis (i.e., manually defining every \n",
    "combination of inputs that will be evaluated) but the real power of EMAT comes from runnning the model\n",
    "using algorithm-created experimental designs.\n",
    "\n",
    "An important experimental design used in exploratory modeling is the Latin Hypercube.  This design selects\n",
    "a random set of experiments across multiple input dimensions, to ensure \"good\" coverage of the \n",
    "multi-dimensional modeling space.\n",
    "\n",
    "The `design_latin_hypercube` function creates such a design based on a `Scope`, and optionally\n",
    "stores the design of experiments in a database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emat.experiment.experimental_design import design_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design = design_experiments(road_scope, db=emat_db, n_samples_per_factor=10, sampler='lhs')\n",
    "design.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_design = design_experiments(road_scope, db=emat_db, n_samples=5000, sampler='lhs', design_name='lhs_large')\n",
    "large_design.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can review what experimental designs have already been stored in the database using the \n",
    "`read_design_names` method of the `Database` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emat_db.read_design_names('EMAT Road Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Model in Python"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Up until this point, we have been considering a model in the abstract, defining in the :class:`Scope` what the inputs \n",
    "and outputs will be, and designing some experiments we would like to run with the model.  Now we will actually \n",
    "interface with the model itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition\n",
    "\n",
    "In the simplest approach for EMAT, a model can be defined as a basic Python function, which accepts all\n",
    "inputs (exogenous uncertainties, policy levers, and externally defined constants) as named keyword\n",
    "arguments, and returns a dictionary where the dictionary keys are names of performace measures, and \n",
    "the mapped values are the computed values for those performance measures.  The `Road_Capacity_Investment`\n",
    "function provided in EMAT is an example of such a function.  This made-up example considers the \n",
    "investment in capacity expansion for a single roadway link.  The inputs to this function are described\n",
    "above in the Scope, including uncertain parameters in the volume-delay function,\n",
    "traffic volumes, value of travel time savings, unit construction costs, and interest rates, and policy levers including the \n",
    "amount of capacity expansion and amortization period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emat.model.core_python import PythonCoreModel\n",
    "from emat.model.core_python import Road_Capacity_Investment"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "The :class:`PythonCoreModel <emat.model.core_python.core_python_api.PythonCoreModel>` object \n",
    "provides an interface that links the basic Python function that represents \n",
    "the model, the :class:`Scope <emat.scope.scope.Scope>`, and optionally the \n",
    ":class:`Database <emat.database.database.Database>` used to manage data storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = PythonCoreModel(Road_Capacity_Investment, scope=road_scope, db=emat_db)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "From the :class:`PythonCoreModel`, which links the model, scope, design, and database, we can run the design of experiments.  \n",
    "This will systematically run the core model with each set of input parameters in the design, store the results in\n",
    "the database, and return a pandas.DataFrame containing the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench import SequentialEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SequentialEvaluator(m) as eval_seq:\n",
    "    lhs_results = m.run_experiments(design_name='lhs', evaluator=eval_seq)\n",
    "lhs_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running a large number of experiments, it may be valuable to parallelize the \n",
    "processing using a DistributedEvaluator instead of the SequentialEvaluator.\n",
    "The DistributedEvaluator uses dask.distributed to distribute the workload to\n",
    "a cluster of processes, which can all be on the same machine or distributed\n",
    "over multiple networked computers. (The details of using dask.distributed in \n",
    "more complex environments are beyond this scope of this example, but interested\n",
    "users can refer to that package's [documentation](https://distributed.dask.org/).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_results = m.run_experiments(design_name='lhs_large')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a particular design has been run once, the results can be recovered from the database without re-running the model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_results = m.read_experiments(design_name='lhs')\n",
    "reload_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to load only the parameters, or only the performance meausures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lhs_params = m.read_experiment_parameters(design_name='lhs')\n",
    "lhs_params.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lhs_outcomes = m.read_experiment_measures(design_name='lhs')\n",
    "lhs_outcomes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.get_feature_scores('lhs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scenario discovery in exploratory modeling is focused on finding scenarios that are interesting to the user.  \n",
    "The process generally begins through the identification of particular outcomes that are \"of interest\",\n",
    "and the discovery process that can seek out what factor or combination of factors can result in\n",
    "those outcomes.\n",
    "\n",
    "There are a variety of methods to use for scenario discovery.  We illustrate a few here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRIM\n",
    "\n",
    "Patient rule induction method (PRIM) is an algorithm that operates on a set of data with inputs and outputs.  \n",
    "It is used for locating areas of an outcome space that are of particular interest, which it does by reducing \n",
    "the data size incrementally by small amounts in an iterative process as follows:\n",
    "    \n",
    "- Candidate boxes are generated.  These boxes represent incrementally smaller sets of the data.  \n",
    "  Each box removes a portion of the data based on the levels of a single input variable.\n",
    "  * For categorical input variables, there is one box generated for each category with each box \n",
    "    removing one category from the data set.\n",
    "  * For integer and continuous variables, two boxes are generated â€“ one box that removes a \n",
    "    portion of data representing the smallest set of values for that input variable and another \n",
    "    box that removes a portion of data representing the largest set of values for that input.  \n",
    "    The step size for these variables is controlled by the analyst.\n",
    "- For each candidate box, the relative improvement in the number of outcomes of interest inside \n",
    "  the box is calculated and the candidate box with the highest improvement is selected.\n",
    "- The data in the selected candidate box replaces the starting data and the process is repeated.\n",
    "\n",
    "The process ends based on a stopping criteria.  For more details on the algorithm, \n",
    "see [Friedman and Fisher (1999)](http://statweb.stanford.edu/~jhf/ftp/prim.pdf) or \n",
    "[Kwakkel and Jaxa-Rozen (2016)](https://www.sciencedirect.com/science/article/pii/S1364815215301092).\n",
    "\n",
    "The PRIM algorithm is particularly useful for scenario discovery, which broadly is the process of \n",
    "identifying particular scenarios of interest in a large and deeply uncertain dataset.   \n",
    "In the context of exploratory modeling, scenario discovery is often used to obtain a better understanding \n",
    "of areas of the uncertainty space where a policy or collection of policies performs poorly because it is \n",
    "often used in tandem with robust search methods for identifying policies that perform well \n",
    "([Kwakkel and Jaxa-Rozen (2016)](https://www.sciencedirect.com/science/article/pii/S1364815215301092))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emat.analysis import prim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = m.read_experiment_parameters(design_name='lhs_large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.read_experiment_measures(design_name='lhs_large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prim_alg = prim.Prim(\n",
    "    m.read_experiment_parameters(design_name='lhs_large'), \n",
    "    m.read_experiment_measures(design_name='lhs_large')['net_benefits']>0, \n",
    "    threshold=0.4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box1 = prim_alg.find_box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This interactive version doesn't render well in documentation\n",
    "# tradeoff = box1.inspect_tradeoff()\n",
    "# tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Show(box1.show_tradeoff())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box1.inspect(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CART\n",
    "\n",
    "Classification and Regression Trees (CART) can also be used for scenario discovery. \n",
    "They partition the explored space (i.e., the scope) into a number of sections, with each partition\n",
    "being added in such a way as to maximize the difference between observations on each \n",
    "side of the newly added partition divider, subject to some constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench.analysis import cart\n",
    "\n",
    "cart_alg = cart.CART(\n",
    "    m.read_experiment_parameters(design_name='lhs_large'), \n",
    "    m.read_experiment_measures(design_name='lhs_large')['net_benefits']>0,\n",
    ")\n",
    "cart_alg.build_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Show(cart_alg.show_tree(format='svg')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart_alg.boxes_to_dataframe(include_stats=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Meta-Model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Creating a meta-model requires an existing model, plus a set of \n",
    "experiments (including inputs and outputs) run with that model\n",
    "to support the estimation of the meta-model.  After having completed\n",
    "sufficient initial runs of the core model, instantiating a meta-model\n",
    "is as simple as calling a `create_metamodel_*` method on the core\n",
    "model, either giving a design of experiments stored in the database\n",
    "with :meth:`create_metamodel_from_design <emat.AbstractCoreModel.create_metamodel_from_design>`\n",
    "or passing the experimental results directly with\n",
    ":meth:`create_metamodel_from_data <emat.AbstractCoreModel.create_metamodel_from_data>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = m.create_metamodel_from_design('lhs')\n",
    "mm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the performance of the meta-model, we can create an\n",
    "alternate design of experiments.  Note that to get different random values,\n",
    "we set the `random_seed` argument to something other than the default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design2 = design_experiments(road_scope, db=emat_db, n_samples_per_factor=10, sampler='lhs', random_seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design2_results = mm.run_experiments(design2)\n",
    "design2_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm.function.cross_val_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Core vs Meta Model Results\n",
    "\n",
    "We can generate a variety of plots to compare the distribution of meta-model outcomes\n",
    "on the new design against the original model's results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Show(scatter_graph(\n",
    "    X=[ design2_results['input_flow'],\n",
    "        lhs_results['input_flow'] ],\n",
    "    Y=[ design2_results['time_savings'],\n",
    "        lhs_results['time_savings'],  ],\n",
    "    legend_labels=[ 'meta-model',\n",
    "                    'core model',  ]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Show(scatter_graph(\n",
    "    X=[ design2_results['no_build_travel_time'],\n",
    "        lhs_results['no_build_travel_time'], ],\n",
    "    Y=[ design2_results['time_savings'],\n",
    "        lhs_results['time_savings'], ],\n",
    "    legend_labels=[ 'meta-model',\n",
    "                    'core model', ]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Show(scatter_graph(\n",
    "    X=[ design2_results['expand_capacity'],\n",
    "        lhs_results['expand_capacity'], ],\n",
    "    Y=[ design2_results['present_cost_expansion'],\n",
    "        lhs_results['present_cost_expansion'], ],\n",
    "    legend_labels=[ 'meta-model',\n",
    "                    'core model', ]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Evaluating Meta-Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = design_experiments(road_scope, n_samples=1000, sampler='ulhs', db=None, random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_std = mm.function.compute_std(background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure= mm.function(background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure_std = background.apply(\n",
    "#     lambda x: pandas.Series(mm.function.compute_std(**x)),\n",
    "#     axis=1,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# background.join(measure_std.add_suffix(\"_std\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    background.join(pandas.qcut(measure.net_benefits, 5).rename(\"sector\")),\n",
    "    hue='sector',\n",
    "    vars=['alpha','beta','amortization_period']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    background.join(measure_std.add_suffix(\"_std\")).join(pandas.qcut(measure_std.net_benefits, 5).rename(\"sector\")),\n",
    "    hue='sector',\n",
    "    vars=['alpha','beta','amortization_period']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draws = design_experiments(road_scope, n_samples=2000, sampler='lhs', db=None, random_seed=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawx = mm.run_experiments(draws, db=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawx_std = mm.function.compute_std(draws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prim_alg = prim.Prim(\n",
    "    draws, \n",
    "    drawx['net_benefits']>0, \n",
    "    threshold=0.4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box1 = prim_alg.find_box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tradeoff = box1.inspect_tradeoff()\n",
    "tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 31\n",
    "box1.inspect(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box1.box_lims[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box1.select(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box1.yi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawx_std.drop(box1.yi).net_benefits.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawx_std.iloc[box1.yi].net_benefits.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = box1.to_emat_box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb.scope = road_scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb.uncertainty_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb.lever_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draws_bb = design_experiments(bb, n_samples=2000, sampler='lhs', db=None, random_seed=46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draws_bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emat.util.distributions import truncated, get_distribution_bounds\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncertainties(self):\n",
    "    \"\"\"Get a list of exogenous uncertainties.\"\"\"\n",
    "    result = []\n",
    "    thresh = self.uncertainty_thresholds\n",
    "    for i in self.scope.get_uncertainties():\n",
    "        i = copy.deepcopy(i)\n",
    "        if i.name in thresh:\n",
    "            lowerbound, upperbound = thresh[i.name]\n",
    "            if lowerbound is None:\n",
    "                lowerbound = -numpy.inf\n",
    "            if upperbound is None:\n",
    "                upperbound = numpy.inf\n",
    "            i.dist = truncated(i.dist, lowerbound, upperbound)\n",
    "            i.lower_bound, i.upper_bound = get_distribution_bounds(i.dist)\n",
    "        result.append(i)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = [(i, i.name, i.lower_bound, i.upper_bound) for i in get_uncertainties(bb)]\n",
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di = j[2][0].dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di.ppf(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di.ppf(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.ppf(0), dt.ppf(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = numpy.linspace(2,7, 1000)\n",
    "plt.plot(x, di.pdf(x))\n",
    "plt.plot(x, dt.pdf(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(road_scope.dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:EMAT-DEV]",
   "language": "python",
   "name": "conda-env-EMAT-DEV-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
